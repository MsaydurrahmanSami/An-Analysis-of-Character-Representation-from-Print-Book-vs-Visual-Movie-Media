# OurGoal

Our goal is Given an input video to select a  bunch of frames to create a summary video that can capture the vital information of the input video. With many books getting adapted as movies, video summarization provides a useful tool that assists video analysis. In this paper, we formulate a video movie summarization to determine the importance of any character in movies with their book counterparts. We propose a fully convolutional sequence network model classifier to solve the video summarization problem, a search model to analyze book representation and statistical analysis for comparison.

Review-

1.Screen time of an actor in a movie or an episode is very important. Many actors get paid according to their total screen time. Moreover, we also want to know how much time our favorite character acted on screen.With the advancement of deep learning now its possible to solve various difficult problems.To solve any problem with deep learning, the first requirement is the data.[https://theailearner.com/2019/03/24/calculating-screen-time-of-an-actor-using-deep-learning/]

2.More than 80% of internet traffic will be real data except in particular, it could be video data[3]. It gives us a certain intuitive sense of how much video data there and how many tools to manage the massive amount of video data. Video recorded in one day could have 5 minutes of video summarization instead of 24 hours video. We can use video summarization for thumbnail generation, if we put a mouse around the video on youtube, we will see that the thumbnail option for the summary of the video, which includes the main video is for. A fair bit of the summarization system can automatically generate those thumbnails. In video Customization, if we put a different video on social media on another platform, they show they have a limitation on how long that video can be. Somehow we need to adjust those accordingly. We can use Video Summarization for a better summary In highlight creation compare it with our text analyzer Before we watch any movie based on our favorite book.[https://openaccess.thecvf.com/content_ECCV_2018/papers/Mrigank_Rochan_Video_Summarization_Using_ECCV_2018_paper.pdf]

3.The idea is nothing but videos are nothing but a collection of a set of images. These images are called frames and can be combined to get the original video. So, a problem related to video data is not that different from an image classification or an object detection problem [https://www.analyticsvidhya.com/blog/2018/09/deep-learning-video-classification-python/]

4.In most simplistic terms. BERT stands for Bidirectional Encoder Representation from Transformers. It is a language representation model and basically a trained transformers encoder stack. BERT applies bidirectional training of transformer encoders using Auto-encoder language modeling. It performs two unsupervised learning tasks during pertaining, one is masked language modeling and the other one is next sentence prediction tasks. It finally achieves state-of-the-art results on GLUE, MNLI, SQuAD data-sets. If we look at the architecture of BERT. It consists of 12 stacked encoder units in the base model and 24 stacked encoder units in the large model. Each encoder unit has a similar architecture to the Transformer model which is an attention-based model having encoder-decoder architecture. Coming to the architecture of encoders it consists of two layers one is a multi-headed attention layer and another one is a position-wise fully connected feed-forward neural network. The attention layer calculates attention on the input embedding using this formula which is then added, normalized, and passed to the neural network.

